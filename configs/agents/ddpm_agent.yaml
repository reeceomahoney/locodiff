_target_: locodiff.agent.Agent
_recursive_: false

optimization:
  _target_: torch.optim.AdamW
  lr: 1e-4
  betas: [0.9, 0.999]

lr_scheduler:
  _target_: torch.optim.lr_scheduler.CosineAnnealingLR
  T_max: ${max_train_steps}

max_train_steps: ${max_train_steps}
eval_every_n_steps: ${eval_every_n_steps}
num_sampling_steps: ${n_timesteps}
sigma_data: ${sigma_data}
sigma_min: 0.001
sigma_max: 80
use_ema: ${use_ema}
decay: ${decay}
device: ${device}
update_ema_every_n_steps: ${update_ema_every_n_steps}

obs_dim: ${obs_dim}
action_dim: ${action_dim}
skill_dim: ${skill_dim}
T: ${T}
T_cond: ${T_cond}
T_action: ${T_action}
num_envs: ${env.num_envs}
sim_every_n_steps: ${sim_every_n_steps}
weight_decay: ${weight_decay}
cond_lambda: ${cond_lambda}
cond_mask_prob: ${cond_mask_prob}
evaluating: ${evaluating}
reward_fn: ${reward_fn}
ddpm: True

dataset_fn:
  _target_: locodiff.dataloader.get_dataloaders_and_scaler
  data_directory: ${data_path}
  obs_dim: ${obs_dim}
  train_fraction: 0.95
  device: ${device}
  T_cond: ${T_cond}
  T: ${T}
  train_batch_size: 1024
  test_batch_size: 1024
  num_workers: 4
  return_horizon: ${reward_horizon}
  reward_fn: ${reward_fn}

model:
  _target_: locodiff.transformer.DiffusionTransformer
  obs_dim: ${obs_dim}
  skill_dim: ${skill_dim}
  act_dim: ${action_dim}
  d_model: ${hidden_dim}
  nhead: ${n_heads}
  num_layers: ${num_hidden_layers}
  T: ${T}
  T_cond: ${T_cond}
  device: ${device}
  cond_mask_prob: ${cond_mask_prob}
  dropout: ${dropout}
  ddpm: True

noise_scheduler:
  _target_: diffusers.schedulers.scheduling_ddpm.DDPMScheduler
  num_train_timesteps: 10
  beta_start: 0.0001
  beta_end: 0.02
  beta_schedule: squaredcos_cap_v2
  variance_type: fixed_small # Yilun's paper uses fixed_small_log instead, but easy to cause Nan
  clip_sample: True # required when predict_epsilon=False
  prediction_type: epsilon # or sample
